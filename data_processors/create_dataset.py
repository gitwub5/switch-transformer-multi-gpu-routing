import re
import random
import os
import json
import matplotlib.pyplot as plt
import seaborn as sns
from datasets import load_dataset
from transformers import AutoTokenizer
from collections import defaultdict

# âœ… ì‚¬ìš©ì ì„¤ì •
MAX_SAMPLES = 5000
MIN_TOKEN_LENGTH = 1
MAX_TOKEN_LENGTH = 512
BUCKET_WIDTH = 10  # 10ë‹¨ìœ„ ë²„í‚·
tokenizer = AutoTokenizer.from_pretrained("google/switch-base-8")

# âœ… ë²„í‚· ì„¸íŒ…
num_buckets = (MAX_TOKEN_LENGTH - MIN_TOKEN_LENGTH + 1) // BUCKET_WIDTH
samples_per_bucket = MAX_SAMPLES // num_buckets
buckets = defaultdict(list)

# âœ… ì „ì²˜ë¦¬ í•¨ìˆ˜
def process_dataset(dataset, text_key):
    for ex in dataset:
        text = ex[text_key].strip().replace("\n", " ")
        tokens = tokenizer.encode(text, add_special_tokens=True)
        token_len = len(tokens)
        if MIN_TOKEN_LENGTH <= token_len <= MAX_TOKEN_LENGTH:
            bucket_id = (token_len - MIN_TOKEN_LENGTH) // BUCKET_WIDTH
            if len(buckets[bucket_id]) < samples_per_bucket:
                buckets[bucket_id].append({
                    "text": text,
                    "token_length": token_len
                })
        if sum(len(lst) for lst in buckets.values()) >= MAX_SAMPLES:
            break

# âœ… ë°ì´í„° ìˆ˜ì§‘
print("ğŸ“¦ AG News ìˆ˜ì§‘ ì¤‘...")
ag_dataset = load_dataset("ag_news", split="train[:100000]")
process_dataset(ag_dataset, "text")

print("ğŸ“¦ OpenWebText ìˆ˜ì§‘ ì¤‘...")
owt_dataset = load_dataset("openwebtext", split="train[:100000]")
process_dataset(owt_dataset, "text")

# âœ… ê²°ê³¼ ì •ë¦¬
texts = []
for bucket in buckets.values():
    texts.extend(bucket)

random.shuffle(texts)
texts = texts[:MAX_SAMPLES]
texts = [{"text_id": f"S{i+1:04d}", "text": entry["text"], "token_length": entry["token_length"]} for i, entry in enumerate(texts)]

# âœ… í‰ê·  í† í° ìˆ˜
avg_tokens = sum(t["token_length"] for t in texts) / len(texts)
print(f"âœ… ì´ ë¬¸ì¥ ìˆ˜: {len(texts)} | í‰ê·  í† í° ìˆ˜: {avg_tokens:.2f}")

# âœ… ì €ì¥
os.makedirs("data", exist_ok=True)
output_path = f"data/structured_texts_balanced_{MAX_SAMPLES}.json"
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(texts, f, ensure_ascii=False, indent=2)
print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")

# âœ… ì‹œê°í™”
sns.set(style="whitegrid")
plt.figure(figsize=(12, 6))
token_lengths = [t["token_length"] for t in texts]
sns.histplot(token_lengths, bins=50, kde=True, color="skyblue")
plt.title("Token Length Distribution (Balanced Sample)")
plt.xlabel("Token Count")
plt.ylabel("Frequency")
plt.tight_layout()
plt.savefig("data/token_length_distribution.png")