import os
import joblib
import numpy as np
import pandas as pd
from xgboost import XGBRegressor
import matplotlib.pyplot as plt
import torch

# =========================
# í•™ìŠµ ê²°ê³¼ ì €ì¥
# =========================
def save_model_summary(summary_records, output_base_dir):
    """
    í•™ìŠµëœ ëª¨ë“  ëª¨ë¸ì˜ ì„±ëŠ¥ ìš”ì•½ì„ CSVë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    """
    summary_df = pd.DataFrame(summary_records)
    summary_dir = os.path.join(output_base_dir, "results")
    os.makedirs(summary_dir, exist_ok=True)

    summary_path = os.path.join(summary_dir, "training_summary.csv")
    summary_df.to_csv(summary_path, index=False)
    print(f"ğŸ“„ ì „ì²´ í•™ìŠµ ìš”ì•½ ì €ì¥ ì™„ë£Œ: {summary_path}")

# =========================
# ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
# =========================
def load_and_preprocess_data(csv_path):
    df = pd.read_csv(csv_path)
    df["gpu_id_encoded"] = df["gpu_id"].map({"1080Ti": 0, "A6000": 1})
    
    # ì´ìƒì¹˜ ì œê±°ë¥¼ ìœ„í•œ IQR ë°©ë²• ì ìš©
    def remove_outliers(group):
        Q1 = group['latency_ms'].quantile(0.25)
        Q3 = group['latency_ms'].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        return group[(group['latency_ms'] >= lower_bound) & (group['latency_ms'] <= upper_bound)]
    
    # layer_type, layer_index, expert_id, gpu_id_encoded ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì´ìƒì¹˜ ì œê±°
    df = df.groupby(['layer_type', 'layer_index', 'expert_id', 'gpu_id_encoded']).apply(remove_outliers).reset_index(drop=True)
    
    print(f"ğŸ“Š ì´ìƒì¹˜ ì œê±° í›„ ë°ì´í„° í¬ê¸°: {len(df)} rows")
    return df


# ============================
# layer_type, layer_index ê¸°ì¤€ í•™ìŠµ
# ============================
def train_and_save_models(df, output_base_dir):
    summary_records = []
    
    for layer_type in df["layer_type"].unique():
        type_df = df[df["layer_type"] == layer_type]
        type_str = "encoder" if layer_type == "encoder" else "decoder"

        for layer_index in sorted(type_df["layer_index"].unique()):
            print(f"ğŸ” {type_str} layer {layer_index} í•™ìŠµ ì‹œì‘")
            layer_df = type_df[type_df["layer_index"] == layer_index]
            layer_dir = os.path.join(output_base_dir, type_str)
            os.makedirs(layer_dir, exist_ok=True)

            # ì…ë ¥ íŠ¹ì„± (token_id ì œì™¸)
            features = [
                "router_entropy", "router_score",
                "layer_token_count", "gpu_id_encoded", "expert_id"
            ]
            
            X = layer_df[features]
            y = layer_df["latency_ms"]
            
            model = XGBRegressor(device="cuda", n_estimators=100, learning_rate=0.01, max_depth=3)
            model.fit(X, y)

            # ì„±ëŠ¥ í‰ê°€
            preds = model.predict(X)
            mae = np.mean(np.abs(preds - y))
            rmse = np.sqrt(np.mean((preds - y) ** 2))
            r2 = model.score(X, y)

            summary_records.append({
                "layer_type": type_str,
                "layer_index": layer_index,
                "sample_count": len(layer_df),
                "MAE": round(mae, 4),
                "RMSE": round(rmse, 4),
                "R2": round(r2, 4)
            })

            # layerë³„ë¡œ í•˜ë‚˜ì˜ ëª¨ë¸ ì €ì¥
            fname = f"layer_{layer_index}.joblib"
            joblib.dump(model, os.path.join(layer_dir, fname))
            print(f"âœ… {type_str} layer {layer_index} í•™ìŠµ ì™„ë£Œ")
            
    save_model_summary(summary_records, output_base_dir)

# ============================
# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
# ============================
def load_all_models(model_base_dir):
    models = {}

    for layer_type in ["encoder", "decoder"]:
        type_path = os.path.join(model_base_dir, layer_type)
        if not os.path.exists(type_path):
            continue

        for fname in os.listdir(type_path):
            if fname.endswith(".joblib"):
                # íŒŒì¼ ì´ë¦„ í˜•ì‹: layer_{layer_index}.joblib
                layer_index = int(fname.replace("layer_", "").replace(".joblib", ""))
                model = joblib.load(os.path.join(type_path, fname))
                models[(layer_type, layer_index)] = model

    return models



# ============================
# ì¶”ë¡ : ìµœì  GPU ì„ íƒ
# ============================
def latency_aware_gpu_selector(token_feature: dict, expert_id: int, models: dict):
    """
    ì£¼ì–´ì§„ expert_idì— ëŒ€í•´ latencyê°€ ê°€ì¥ ë‚®ì€ GPUë¥¼ ì„ íƒí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        token_feature (dict): í† í°ì˜ íŠ¹ì„± ì •ë³´
            - router_entropy: ë¼ìš°í„° ì—”íŠ¸ë¡œí”¼
            - router_score: ë¼ìš°í„° ì ìˆ˜
            - layer_token_count: layerì˜ í† í° ìˆ˜
            - layer_type_encoded: layer íƒ€ì… (0: encoder, 1: decoder)
            - gpu_id_encoded: í˜„ì¬ í† í°ì´ ìœ„ì¹˜í•œ GPU (0: 1080Ti, 1: A6000)
        expert_id (int): ê¸°ì¡´ ë¼ìš°í„°ê°€ ì„ íƒí•œ expert ID
        models (dict): í•™ìŠµëœ ëª¨ë¸ë“¤
    
    Returns:
        dict: ì„ íƒëœ GPU ì •ë³´ì™€ ì˜ˆìƒ latency
    """
    # í˜„ì¬ layer -> ë‹¤ìŒ routing ê°€ëŠ¥í•œ layer ì •ì˜
    NEXT_ROUTING_LAYER = {
        1: 1,  # layer 1ì˜ expert ì„ íƒ
        3: 3,  # layer 3ì˜ expert ì„ íƒ
        5: 5,  # layer 5ì˜ expert ì„ íƒ
        7: 7,  # layer 7ì˜ expert ì„ íƒ
        9: 9,  # layer 9ì˜ expert ì„ íƒ
        11: 11,  # layer 11ì˜ expert ì„ íƒ
    }

    # í˜„ì¬ layer ê¸°ì¤€ ë‹¤ìŒ layer ì„¤ì •
    current_layer = token_feature["layer_index"]
    current_type = "encoder" if token_feature["layer_type_encoded"] == 0 else "decoder"
    
    # encoder layer 11ì—ì„œ decoder layer 1ë¡œ ë„˜ì–´ê°€ëŠ” ê²½ìš°
    if current_type == "encoder" and current_layer == 11:
        next_layer_index = 1
        next_type = "decoder"
    else:
        next_layer_index = NEXT_ROUTING_LAYER.get(current_layer)
        next_type = current_type

    # ë‹¤ìŒ ë¼ìš°íŒ… layerê°€ ì—†ë‹¤ë©´ None ë°˜í™˜
    if next_layer_index is None:
        print(f"â›”ï¸ layer {current_layer}ì—ì„œëŠ” ë” ì´ìƒ routingì´ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # ê° GPUì— ëŒ€í•´ ì˜ˆì¸¡
    best_latency = float("inf")
    best_gpu = None

    for target_gpu_id in [0, 1]:  # 2ê°œì˜ GPU
        # íŠ¹ì„± ë²¡í„° ìƒì„± (token_id ì œì™¸)
        feature_vec = [
            token_feature["router_entropy"],
            token_feature["router_score"],
            token_feature["layer_token_count"],
            expert_id,  # expert_id
            target_gpu_id  # target GPU
        ]
        
        model_key = (next_type, next_layer_index)
        if model_key in models:
            pred_latency = models[model_key].predict([feature_vec])[0]
            pred_latency = max(0.0, pred_latency)
            
            if pred_latency < best_latency:
                best_latency = pred_latency
                best_gpu = target_gpu_id

    if best_gpu is None:
        return None
    
    return {
        "layer_index": next_layer_index,
        "expert_id": expert_id,
        "gpu_id": best_gpu,
        "latency": float(best_latency)
    }

# ============================
# ì‹¤í–‰ ì˜ˆì‹œ
# ============================
if __name__ == "__main__":
    CSV_PATH = "outputs/router_dataset.csv"
    MODEL_DIR = "models"
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ ì¥ì¹˜: {device}")

    df = load_and_preprocess_data(CSV_PATH)
    print(f"ğŸ“Š ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(df)} rows")
    train_and_save_models(df, MODEL_DIR)

    models = load_all_models(MODEL_DIR)
    print(models.keys())

    example_input = {
        "router_entropy": 0.32,
        "router_score": 0.5,
        "layer_token_count": 128,
        "layer_type_encoded": 0,  # encoder
        "gpu_id_encoded": 0,  # í˜„ì¬ 1080Tiì— ìœ„ì¹˜
        "layer_index": 3
    }

    # ì˜ˆì‹œ: expert_idê°€ 3ìœ¼ë¡œ ê²°ì •ëœ ê²½ìš°
    selected_expert_id = 3
    result = latency_aware_gpu_selector(example_input, selected_expert_id, models)
    
    if result:
        print(f"ğŸ”€ GPU ì„ íƒ ê²°ê³¼:")
        print(f"  - Layer: {result['layer_index']}")
        print(f"  - Expert: {result['expert_id']}")
        print(f"  - GPU: {result['gpu_id']} ({'1080Ti' if result['gpu_id'] == 0 else 'A6000'})")
        print(f"  - ì˜ˆìƒ ì§€ì—°ì‹œê°„: {result['latency']:.6f} ms")
    else:
        print("âŒ GPU ì„ íƒ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
