import torch
import torch.nn as nn
import torch.nn.functional as F

class LatencyMLP(nn.Module):
    def __init__(self, input_dim=8):  # ì…ë ¥ ì°¨ì› ì£¼ì˜
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

    def forward(self, x):
        return self.model(x)
    
class LatencyAwareTop1Router(SwitchTransformersTop1Router):
    def __init__(self, input_dim, latency_model_path, gpu_mapping_dict, **kwargs):
        super().__init__(**kwargs)
        self.latency_model = LatencyMLP(input_dim)
        self.latency_model.load_state_dict(torch.load(latency_model_path))
        self.latency_model.eval().cuda()
        self.gpu_mapping_dict = gpu_mapping_dict  # {expert_id: gpu_id}

    def forward(self, hidden_states: torch.Tensor, **kwargs):
        # âœ… 1. ê¸°ì¡´ ë¼ìš°íŒ… ë¡œì§ ìœ ì§€
        expert_index, router_probs, router_logits = super().forward(hidden_states)

        # âœ… 2. ë¼ìš°íŒ… ì •ë³´ ê¸°ë°˜ í† í°ë³„ latency-aware GPU ì„ íƒ ë¡œì§
        batch_size, seq_len, _ = hidden_states.shape
        selected_gpu_ids = torch.zeros(batch_size, seq_len, dtype=torch.int32)

        # í˜„ì¬ GPU ID í™•ì¸
        device_name = torch.cuda.get_device_name(0)  # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ GPU ì´ë¦„
        if "1080" in device_name:
            gpu_id_encoded = 0  # 1080Ti
        else:
            gpu_id_encoded = 1  # A6000

        for i in range(batch_size):
            for j in range(seq_len):
                # ì–´ë–¤ expertë¡œ ë¼ìš°íŒ…ë˜ì—ˆëŠ”ì§€ í™•ì¸
                expert_id = torch.argmax(expert_index[i, j]).item()
                router_score = router_probs[i, j].item()
                entropy = -torch.sum(router_logits[i, j] * torch.log_softmax(router_logits[i, j], dim=-1)).item()

                # ğŸ”§ feature vector êµ¬ì„±
                features = torch.tensor([
                    0.0,  # token_id ìë¦¬ (token_idê°’ ë°›ì•„ì™€ì•¼í•¨)
                    float(expert_id), # expert_id
                    float(kwargs.get("layer_index", 0)), # layer_index
                    float(entropy), # router_entropy
                    float(router_score), # router_score
                    float(kwargs.get("layer_token_count", 128)), # layer_token_count
                    float(kwargs.get("layer_type_encoded", 0)), # layer_type_encoded
                    float(gpu_id_encoded) # gpu_id_encoded (í˜„ì¬ gpu ìœ„ì¹˜)
                ]).unsqueeze(0).cuda()

                with torch.no_grad():
                    predicted_latency = self.latency_model(features).item()

                # ğŸ” ì˜ˆì‹œ: expert-to-GPU mapping ì‚¬ìš©
                selected_gpu_ids[i, j] = self.gpu_mapping_dict.get(expert_id, 0)

        # ğŸŸ¡ ì„ íƒëœ GPU IDëŠ” ë³„ë„ ë°˜í™˜ (ë˜ëŠ” routing tableì—ì„œ í™œìš©)
        return expert_index, router_probs, router_logits, selected_gpu_ids